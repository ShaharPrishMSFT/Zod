"""
git_procedure.py - LLM-powered English-to-git CLI agent

USAGE:
    python procedures/git_procedure.py "Delete the branch 'feature-x'"
    python procedures/git_procedure.py --smart "Show me all commits made by Alice in the last 2 weeks."
    python procedures/git_procedure.py --no-local "Create a new branch called hotfix and push it to origin"
    python procedures/git_procedure.py --log-stages --smart "Find all files changed in the last 3 commits"

OPTIONS:
    --smart         Retry with OpenAI GPT-4.1-nano if the first model fails
    --no-local      Only print the mapped git command, do not execute it locally
    --log-stages    Enable detailed stage logging
    --print-only    Only print the mapped git command, do not execute it
    --capture-output  Print only the output of the git command (for test harnesses)

NOTES:
- The tool assumes the current working directory is a git repository unless otherwise specified in the instruction.
- All answers are generated by the LLM; no hardcoded mappings or fallbacks are used.

EXAMPLES:
    python procedures/git_procedure.py "git status"
    python procedures/git_procedure.py --smart "Delete the branch 'feature-x'"
    python procedures/git_procedure.py --no-local "Create a new branch called hotfix and push it to origin"
"""

import sys
import subprocess
import time
from pathlib import Path

# Import FormalAI SDK
SDK_PATH = (Path(__file__).parent / "../src/client/python").resolve()
sys.path.insert(0, str(SDK_PATH))
from FormalAiSdk.models.litellm_executor import LiteLLMExecutor
from FormalAiSdk.sdk.session import ModelSession

import argparse

def stage_log(stage, message, enabled):
    if enabled:
        print(f"[STAGE: {stage}] {message}", file=sys.stderr)

# --- BEGIN KNOWLEDGE BASES ---

# Basic Git knowledge base for small model
BASIC_GIT_KB = """
Git Command-Line Quick Reference

Setup:
  git config --global user.name "Your Name"
  git config --global user.email "your@email.com"
  git init
  git clone <url>

Working with Changes:
  git status
  git add <file>
  git commit -m "message"
  git diff

Branching:
  git branch
  git checkout <branch>
  git checkout -b <new-branch>
  git merge <branch>

Remotes:
  git remote add origin <url>
  git fetch origin
  git pull origin main
  git push origin main

History:
  git log --oneline --graph
  git show HEAD~1

Undo:
  git reset --soft HEAD~1
  git revert <commit>
  git commit --amend
  git checkout -- <file>
"""

# Full Git cheat sheet for smart model (from deletethis.md)
FULL_GIT_KB = """
Git Cheat Sheet (Command-Line Only)

Setup and Configuration:
  git config --global user.name "Alice"
  git config --global user.email "alice@example.com"
  git config --global color.ui auto
  git init my-repo
  cd my-repo
  git clone https://github.com/user/repo.git

Working with Changes:
  git status
  git add file1.txt file2.txt
  git diff
  git diff --staged
  git commit -m "Implement new feature"

Branching and Merging:
  git branch -a
  git branch feature-x
  git checkout feature-x
  git checkout -b hotfix
  git merge main

Remote Repositories:
  git remote add origin https://github.com/user/repo.git
  git fetch origin
  git pull origin main
  git push origin main
  git push --tags

Inspecting History:
  git log --oneline --graph
  git log -p
  git reflog
  git show HEAD~1

Undoing and Rewriting History:
  git reset --soft HEAD~1
  git reset --hard HEAD~1
  git revert abc123
  git commit --amend
  git checkout -- file.txt

Stashing Changes:
  git stash
  git stash list
  git stash apply stash@{0}
  git stash pop
  git stash drop stash@{0}

Conflict Resolution:
  git merge feature-branch
  # resolve conflicts, edit files, git add, git commit
  git merge --abort
  git rebase --abort
"""

# --- END KNOWLEDGE BASES ---

def get_git_command_from_llm(english_instruction, smart_mode=False, log_stages=False):
    """
    Returns the git command string mapped from the English instruction using the LLM.
    If smart_mode is True, will retry with OpenAI GPT-4.1-nano if the first model fails.
    """
    stage_log("before_llm_small", f"Preparing system prompt for small model:\n{BASIC_GIT_KB}", log_stages)
    system_prompt = (
        BASIC_GIT_KB
        + "\n\nYou are a CLI assistant. Given an English instruction, output the corresponding git command only. "
        "Do not explain. Do not add extra text. Output only the git command. Do not number your response. Do not repeat the command."
    )
    executor = LiteLLMExecutor("ollama", "mistral")
    session = ModelSession("user", executor)
    session.add_response("system", system_prompt)
    session.add_response("user", english_instruction)
    stage_log("after_llm_small", f"Prompt sent to small model. Awaiting response...", log_stages)
    fork = session.Fork("git_cmd_fork", "user", english_instruction)
    fork.Answer(session)
    for msg in reversed(session.messages):
        if msg.actor == "git_cmd_fork":
            lines = msg.content.strip().splitlines()
            for line in lines:
                line = line.strip()
                if line and "git " in line:
                    line = line.lstrip("0123456789. )-")
                    line = line.split(";")[0].strip()
                    stage_log("after_llm_small", f"Small model returned: {line}", log_stages)
                    return line
            stage_log("after_llm_small", f"Small model returned: {msg.content.strip()}", log_stages)
            return msg.content.strip()
    # If not found and smart_mode, retry with OpenAI GPT-4.1-nano and full KB
    if smart_mode:
        stage_log("before_llm_smart", f"Preparing system prompt for smart model:\n{FULL_GIT_KB}", log_stages)
        system_prompt = (
            FULL_GIT_KB
            + "\n\nYou are a CLI assistant and a git expert. Given an English instruction, output the corresponding git command only. "
            "Do not explain. Do not add extra text. Output only the git command. Do not number your response. Do not repeat the command."
        )
        executor = LiteLLMExecutor("openai", "gpt-4.1-nano")
        session = ModelSession("user", executor)
        session.add_response("system", system_prompt)
        session.add_response("user", english_instruction)
        stage_log("after_llm_smart", f"Prompt sent to smart model. Awaiting response...", log_stages)
        fork = session.Fork("git_cmd_fork", "user", english_instruction)
        fork.Answer(session)
        for msg in reversed(session.messages):
            if msg.actor == "git_cmd_fork":
                lines = msg.content.strip().splitlines()
                for line in lines:
                    line = line.strip()
                    if line and "git " in line:
                        line = line.lstrip("0123456789. )-")
                        line = line.split(";")[0].strip()
                        stage_log("after_llm_smart", f"Smart model returned: {line}", log_stages)
                        return line
                stage_log("after_llm_smart", f"Smart model returned: {msg.content.strip()}", log_stages)
                return msg.content.strip()
    return None

def main():
    overall_start = time.time()
    parser = argparse.ArgumentParser(description="LLM-powered English-to-git CLI agent")
    parser.add_argument("--smart", action="store_true", help="Retry with OpenAI GPT-4.1-nano if the first model fails")
    parser.add_argument("--log-stages", action="store_true", help="Enable detailed stage logging and capture output automatically")
    parser.add_argument("instruction", nargs="+", help="English instruction to map to git command")
    args = parser.parse_args()

    english_instruction = " ".join(args.instruction).strip()
    git_cmd_str = get_git_command_from_llm(english_instruction, smart_mode=args.smart, log_stages=args.log_stages)
    if not git_cmd_str:
        stage_log("error", "LLM did not return a command.", args.log_stages)
        if args.smart:
            stage_log("error", "Tried both ollama/mistral and OpenAI GPT-4.1-nano, but no command was returned.", args.log_stages)
        sys.exit(1)

    # Always print the mapped command to stdout before running
    print(git_cmd_str)

    # If --log-stages is set, capture output automatically and run the command
    if args.log_stages:
        stage_log("llm_result", f"LLM mapped command: {git_cmd_str}", True)
        import shlex
        git_cmd = shlex.split(git_cmd_str)
        def run_and_time_git_command(cmd, cwd):
            stage_log("before_git", f"Running: {' '.join(cmd)} in {cwd}", True)
            start = time.time()
            proc = subprocess.run(
                cmd,
                cwd=cwd,
                capture_output=True,
                text=True,
                check=False,
            )
            print(proc.stdout, end="")
            end = time.time()
            stage_log("after_git", f"Command took {end - start:.3f} seconds", True)
            return proc.returncode
        repo_dir = Path.cwd()
        if not (repo_dir / ".git").exists():
            stage_log("error", f"Current directory {repo_dir} is not a git repository.", True)
            sys.exit(1)
        run_and_time_git_command(git_cmd, repo_dir)
        overall_end = time.time()
        stage_log("timing", f"Total playground session took {overall_end - overall_start:.3f} seconds", True)

if __name__ == "__main__":
    main()
